## Задача - применить знания на практике


## Проблема 1 - Потребление памяти

От пользователей нашей CRM поступил тикет - не работает экспорт заявок за месяц. Произошло это вскоре после того, как девопсы завернули црм-ку в кубер, наложили какие-то ограничения на использование памяти и вообще изменили конфигурацию сервера. 

### Формируем метрику

Потербление памяти я измерял с помощью гема 'benchmark-memory' (https://github.com/michaelherold/benchmark-memory) таким образом:
```
Benchmark.memory do |x|
  x.report('') do
    yield
    GC.start(full_mark: true, immediate_sweep: true) # принудительный запуск Garbage Collector'а, чтобы минимизировать разброс в результатах
  end
end
```
Первичные результаты выглядели так:

```
100 заявок

1.482M memsize (     1.480M retained)
24.814k objects (    24.793k retained)
50.000  strings (    50.000  retained)


1000 заявок

12.811M memsize (    12.809M retained)
226.587k objects (   226.556k retained)
50.000  strings (    50.000  retained)
```

Как видно, если мы в 10 раз увеличиваем количество заявок, примерно в 10 раз у нас и увеличивается потребление памяти. По описанию тикета (задачи) было ясно, что сервис заваливался при попытке сформировать отчет из 38 тысяч заявок. Если, для грубого счета, взять 12.5 Мб памяти на каждую тысячу заявок, то на весь отчет нам понадобилось бы 475 Мб памяти минимум (38 * 12.5 Мб). А если учесть, что GC может работать не самым эффективным образом, то эта цифра может быть заметно больше.

Например, анализ того же сервиса без принудительного запуска GC, всего для 100 заявок таким образом:
```
Benchmark.memory do |x|
  x.report('') do
    yield             # Garbage Collector не запускаем
  end
end
```
давал такие результаты:
```
100 заявок в один раз

50.966M memsize (     1.445M retained)
850.405k objects (    24.514k retained)
50.000  strings (    50.000  retained)

100 заявок в другой раз

80.554M memsize (   920.363k retained)
 1.303M objects (    17.565k retained)
50.000  strings (    50.000  retained)
```
если здесь взять самый низкий показатель, 50 Мб (отбросим даже дробную часть) для 100 заявок, то есть 500 Мб для 1000, то потенциально сервис может "раздуть" потребление памяти до 18,5 Гб! (38 * 500 = 19_000 Мб =~ 18,5 Гб)

### Оптимизация 1 - формат отчета

Для начала я сравнил потребление памяти для отчета в xsls и в csv. И выяснилось, что последний потребляет где то на 40% меньше памяти. Уточнили у "бизнеса", оказалось, формат для них не принципиален.  
```
После перехода на CSV 
(100 заявок)

852.195k memsize (   849.968k retained)
12.034k objects (    12.007k retained)
50.000  strings (    50.000  retained)
```

### Оптимизация 2 - потоковая запись файла.

Сам по себе процесс у нас построен следующим образом:

1) Менеджер нажимает на кнопку ->
  2) Контроллер запускает воркер для асинхронного выполнения работы ->
    3) Export Worker формирует из переданных ему айдишников и типов сущностей коллекцию, изменяет статус экспорта, и вызывает нужный сервис для формирования отчета, в зависимости от типа сущностей ->
      4) Сервис формирует и сохраняет отчет, либо возвращает ошибки, если те возникли.

При этом функционал предусматривает три типа сущностей: 1) Заявки; 2) PDL контракты; 3) IL контракты;
Для всех трех типов существует свой сервис экспорта, Но основную логику по формированию отчета содержит родительский класс BaseExport, от которого все они наследуют. Отличаются дочерние классы только составом полей и заголовков отчета.

Чтобы избежать раздутия памяти, я решил формировать коллекцию объектов не в воркере (на 3 этапе) с последующей передачей в сервис, а в самом сервисе (на 4 этапе). В итоге код воркера сократился на 64 строки. Методы формирования коллекции я разнес по экспорт-сервисам для каждой сущности, а BaseExport перевел на потоковый режим c помощью `find_each`.

В итоге, следующий замер потребления памяти показал такие результаты:
```
Для 100 заявок:

166.688k memsize (   165.445k retained)
1.729k objects (     1.715k retained)
50.000  strings (    50.000  retained)

Для 1000 заявок:

167.510k memsize (   166.227k retained)
1.734k objects (     1.719k retained)
50.000  strings (    50.000  retained)
```

То есть, как мы и ожидали, память не раздувается, и теперь количество передаваемых в сервис объектов влияет главным образом не на память, а только лишь на время выполнения.

### Защита от регрессии

Для защиты от регрессии написал тесты. Причем, поскольку у нас три почти одинаковых сервиса, которые отличаются только способом формирования коллекции (набором связей, подтягиваемых из базы данных в запросе), формирования заголовков и строк отчета, тесты я написал при помощи shared_examples, чтобы не нарушать принцип DRY.

Тут брошу камень в огород 'rspec-benchmark', гема, который создан, чтобы тестить бенчмарк. По идее, для теста аллоцированной памяти существует специальный хелпер `perform_allocation`, но если передать в блок мало-мальски реальную работу, как наш вызов сервиса, то один этот example будет выполняться несколько минут (вопрос я размещал тут, но до сих пор никто на него не ответил https://stackoverflow.com/questions/60409899/benchmark-rspec-is-stuck-without-any-notifications). Естественно, такой тест выкатывать ни в прод, ни в стейдж нельзя. Тогда я написал свой, небольшой, но работающий вполне шустро хелпер.

spec/rails_helper.rb:
```
def total_memory_allocated(&block)
  return if block.nil?

  result = Benchmark.memory do |x|
    x.report('') do
      yield
      GC.start(full_mark: true, immediate_sweep: true)
    end
  end
  (result.entries.first.measurement.memory.allocated / 1_048_576.0).round(3)
end
```

Что позволило писать такие аккуратные exampl'ы:
```
it 'consumpts not more 200 Kb' do
  expect(total_memory_allocated { call_service }).to be < 0.2
end
```
Теперь, если любой из наших экспортов сломается и начнет пожирать память, мы сразу узнаем об этом из тестов. 

## Проблема 2 - скорость выполнения

Уже на стейдже, когда тестировщики запустили экспорт 20 тысяч заявок, выяснилось, что сервис по-прежнему не раотает. Верней, не работает не по-прежнему (потому что раньше он не работал из-за чрезмерного потребления памяти), но работает так медленно, что процесс в кубере почему-то даже не доживает до конца выполнения.

Как показало тестирование, 100 приложений ExportWorker обрабатывает примерно за 40 секунд, что крайне много. 
38 тысяч апликух с такой скоростью будут обрабатываться больше 4 часов. 
Основную долю работы в воркере занимает ApplicationsManagement::Export.


### Оптимизация 1 - XLSX быстрее, чем CSV?

Разложим этот сервис на составные части и попробуем попрофилировать их. Начнем с записи в файл CSV и в файл XLSX. (посколько именно это было нашей первой оптимизацией)

Оценка бенчмарка (с помощью гемов 'benchmark', 'benchmark-ips' и 'kalibera') с фековыми данными показала, что CSV генерится даже быстрее, чем xls, а значит, не это стало причиной долгой генерации отчета.

Benchmark по времени:
                                    user     system      total        real
XLSX generation with 100 rows    1.310000   0.020000   1.330000 (  1.341428)
CSV generation with 100 rows    0.060000   0.000000   0.060000 (  0.060997)
XLSX generation with 1000 rows   1.320000   0.020000   1.340000 (  1.350205)
CSV generation with 1000 rows   0.060000   0.000000   0.060000 (  0.066761)
XLSX generation with 5000 rows   1.310000   0.020000   1.330000 (  1.335231)
CSV generation with 5000 rows   0.060000   0.000000   0.060000 (  0.069863)

Benchmark по количеству вызовов в секунду:

Comparison:
to_csv_export 100 rows:      377.1 i/s
to_csv_export 1000 rows:       47.9 i/s - 7.86x  (± 0.80) slower
to_xlsx_export 100 rows:       21.2 i/s - 17.78x  (± 2.03) slower
to_csv_export 5000 rows:       12.4 i/s - 30.47x  (± 2.74) slower
to_xlsx_export 1000 rows:        3.4 i/s - 110.67x  (± 10.70) slower
to_xlsx_export 5000 rows:        0.7 i/s - 549.68x  (± 76.09) slower
                   with 99.0% confidence

### Оптимизация 2 - иногда, чтобы сделать лучше, надо не делать вообще

Прогнал сервис через профайлер 'ruby-prof'.
Судя по отчету callgrind, 86.43% времени занимает функция IO::wait_readable

Не сразу удалось понять, что это за функция, и почему она съедает так много времени, но в конце концов я докопался до правды. Видимо, эта функция вызывается тогда, когда программа не может ничего записывать (Output), поскольку ждет ввода (Input). И в нашем случае это происходило при формировании предпоследней ячейки строки, где к заявке применялся метод, который каждый раз обращается в AWS и в зависимости от ответа формирует массив из нескольких строк. И все это лишь для того, чтобы вывести 'Yes' или 'No' да еще и в поле с заголовком 'KYC'. Тут сразу напрашиваются несколько вариантов, как можно оптимизировать данное поведение (завести поле в таблице или иначе определять булевое значение KYC), но как учили нас в Thinknetica, иногда самое лучшее решение - это ничего не делать. Страна сказала, что ей это поле в отчете не обосо то и надо, и мы просто выпилили его оттуда.

Результат - 26 тысяч заявок на стейджинге тестировщики прогнали через экспорт за 1 минуту. При прежней скорости этот объем теоретически мог бы обработаться не ранее, чем за 2 часа 50 минут. Прирост по скорости более чем в 170 раз! При том, что фикс составлял удаление ровно двух строчек - одну из формирования заголовка отчета, другую - из формирования строки. 



